{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import necessary site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy scipy torch matplotlib\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from typing import Type, Tuple, List, Dict, Union, Any\n",
    "import copy\n",
    "import operator\n",
    "from enum import Enum, auto\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some help function\n",
    "\n",
    "Define ***Sparse Dropout*** function and ***transform_matrix_2_Tensor*** function to accelerate computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dropout operation for Sparse Tensor to mitigate overfit quickly.\n",
    "class SparseDropout(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_coal = input.coalesce() # transfer input tensor to COO format.\n",
    "        drop_val = F.dropout(input_coal._values(), self.p, self.training) # only effect the value part _values() of sparse tensor.\n",
    "        return torch.sparse.FloatTensor(input_coal._indices(), drop_val, input.shape) # reset sparse tensor.\n",
    "\n",
    "# Mix dropout operation for Sparse Tensor and Dense Tensor.\n",
    "class MixedDropout(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.dense_dropout = nn.Dropout(p) # standard dropout is okay.\n",
    "        self.sparse_dropout = SparseDropout(p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_sparse:\n",
    "            return self.sparse_dropout(input)\n",
    "        else:\n",
    "            return self.dense_dropout(input)\n",
    "\n",
    "\n",
    "class MixedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_features, out_features)) # Random initialization. [in_features x out_features]\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # Initialize weights and bias.\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, mode='fan_out', a=math.sqrt(5)) # Using Kaiming Uniform.\n",
    "        if self.bias is not None:\n",
    "            _, fan_out = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_out)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            if input.is_sparse: # using sparse matrix multiplication.\n",
    "                res = torch.sparse.mm(input, self.weight)\n",
    "            else: # using standard matrix multiplication.\n",
    "                res = input.matmul(self.weight)\n",
    "        else:\n",
    "            if input.is_sparse: # using sparse matrix multiplication.\n",
    "                res = torch.sparse.addmm(self.bias.expand(input.shape[0], -1), input, self.weight)\n",
    "            else: # using standard matrix multiplication.\n",
    "                res = torch.addmm(self.bias, input, self.weight)\n",
    "        return res\n",
    "\n",
    "# tansform the sparse matrix to a PyTorch Tensor quickly.\n",
    "def sparse_matrix_to_torch(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.array([coo.row, coo.col])\n",
    "    return torch.sparse.FloatTensor(\n",
    "            torch.LongTensor(indices),\n",
    "            torch.FloatTensor(coo.data),\n",
    "            coo.shape)\n",
    "\n",
    "# tansform the sparse matrix to a PyTorch Tensor.\n",
    "def matrix_to_torch(X):\n",
    "    if sp.issparse(X):\n",
    "        return sparse_matrix_to_torch(X)\n",
    "    else:\n",
    "        return torch.FloatTensor(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PPR(Personalized PageRank) Matrix with teleport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes a normalized adjacency matrix with added self-loops.\n",
    "def calc_A_hat(adj_matrix: sp.spmatrix | sp.sparray) -> sp.spmatrix | sp.sparray:\n",
    "    nnodes = adj_matrix.shape[0]\n",
    "    A = adj_matrix + sp.eye(nnodes) # add self-loops.\n",
    "    D_vec = np.sum(A, axis=1) # degrees of each nodes.\n",
    "    if isinstance(A, sp.spmatrix):\n",
    "        D_vec = D_vec.A1\n",
    "    D_vec_invsqrt_corr = 1 / np.sqrt(D_vec)\n",
    "    D_invsqrt_corr = sp.diags(D_vec_invsqrt_corr) # D^{-1/2}\n",
    "    return D_invsqrt_corr @ A @ D_invsqrt_corr # A^{hat} = D^{-1/2} A' D^{1/2}\n",
    "\n",
    "# compute PPR(Personalized PageRank) Matrix exactly\n",
    "def calc_ppr_exact(adj_matrix: sp.spmatrix, alpha: float) -> np.ndarray:\n",
    "    nnodes = adj_matrix.shape[0]\n",
    "    M = calc_A_hat(adj_matrix) # A^{hat}\n",
    "    A_inner = sp.eye(nnodes) - (1 - alpha) * M # (I - (1 - \\alpha) * A^{hat})^{-1}, where \\alpha is teleportation probability\n",
    "    return alpha * np.linalg.inv(A_inner.toarray()) # PPR = \\alpha * ((I - (1 - \\alpha) * A^{hat})^{-1})\n",
    "\n",
    "\n",
    "class PPRExact(nn.Module):\n",
    "    def __init__(self, adj_matrix: sp.spmatrix, alpha: float, drop_prob: float = None):\n",
    "        super().__init__()\n",
    "\n",
    "        ppr_mat = calc_ppr_exact(adj_matrix, alpha) # Compute PPR Matrix exactly.\n",
    "        self.register_buffer('mat', torch.FloatTensor(ppr_mat)) # register in buffer, and transfer it to a Tensor\n",
    "                                         # which is saved with model, but is not participating in backpropagation.\n",
    "        # Using dropout if necessary.\n",
    "        if drop_prob is None or drop_prob == 0:\n",
    "            self.dropout = lambda x: x\n",
    "        else:\n",
    "            self.dropout = MixedDropout(drop_prob)\n",
    "\n",
    "    def forward(self, predictions: torch.FloatTensor, idx: torch.LongTensor):\n",
    "        # output = dropout(PPR[idx]) Ã— predictions\n",
    "        return self.dropout(self.mat[idx]) @ predictions\n",
    "\n",
    "\n",
    "class PPRPowerIteration(nn.Module):\n",
    "    def __init__(self, adj_matrix: sp.spmatrix, alpha: float, niter: int, drop_prob: float = None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.niter = niter\n",
    "\n",
    "        M = calc_A_hat(adj_matrix) # Compute Adjacent matrix A^{hat}.\n",
    "        self.register_buffer('A_hat', sparse_matrix_to_torch((1 - alpha) * M)) # register in buffer, and transfer it to a Tensor\n",
    "                                         # which is saved with model, but is not participating in backpropagation.\n",
    "        # Using dropout if necessary.\n",
    "        if drop_prob is None or drop_prob == 0:\n",
    "            self.dropout = lambda x: x\n",
    "        else:\n",
    "            self.dropout = MixedDropout(drop_prob)\n",
    "\n",
    "    def forward(self, local_preds: torch.FloatTensor, idx: torch.LongTensor):\n",
    "        preds = local_preds\n",
    "        for _ in range(self.niter): # niter times power iteration\n",
    "            # preds = Dropout(A) * preds + alpha * local_preds\n",
    "            A_drop = self.dropout(self.A_hat)\n",
    "            preds = A_drop @ preds + self.alpha * local_preds\n",
    "        return preds[idx]\n",
    "    \n",
    "\n",
    "class NoPPRPowerIteration(nn.Module):\n",
    "    def __init__(self, adj_matrix: sp.spmatrix, drop_prob: float = None):\n",
    "        super().__init__()\n",
    "\n",
    "        M = calc_A_hat(adj_matrix) # Compute Adjacent matrix A^{hat}.\n",
    "        self.register_buffer('A_hat', sparse_matrix_to_torch(M))  # register in buffer, and transfer it to a Tensor\n",
    "                                         # which is saved with model, but is not participating in backpropagation.\n",
    "\n",
    "        # Using dropout if necessary.\n",
    "        if drop_prob is None or drop_prob == 0:\n",
    "            self.dropout = lambda x: x\n",
    "        else:\n",
    "            self.dropout = MixedDropout(drop_prob)\n",
    "\n",
    "    def forward(self, local_preds: torch.FloatTensor, idx: torch.LongTensor):\n",
    "        # Just simple matrix multiplication.\n",
    "        A_drop = self.dropout(self.A_hat)\n",
    "        preds = A_drop @ local_preds\n",
    "        return preds[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPRGNN(nn.Module):\n",
    "    def __init__(self, nfeatures: int, nclasses: int, hiddenunits: List[int], drop_prob: float,\n",
    "                 propagation: nn.Module, bias: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        fcs = [MixedLinear(nfeatures, hiddenunits[0], bias=bias)] # Full connected Layer.\n",
    "        for i in range(1, len(hiddenunits)):\n",
    "            fcs.append(nn.Linear(hiddenunits[i - 1], hiddenunits[i], bias=bias))\n",
    "        fcs.append(nn.Linear(hiddenunits[-1], nclasses, bias=bias))\n",
    "        self.fcs = nn.ModuleList(fcs)\n",
    "\n",
    "        self.reg_params = list(self.fcs[0].parameters()) # regularization parameters.\n",
    "\n",
    "        if drop_prob == 0:\n",
    "            self.dropout = lambda x: x\n",
    "        else:\n",
    "            self.dropout = MixedDropout(drop_prob)\n",
    "        self.act_fn = nn.ReLU()\n",
    "\n",
    "        self.propagation = propagation\n",
    "\n",
    "    def _transform_features(self, attr_matrix: torch.sparse.FloatTensor):\n",
    "        # h_{i+1} = ReLU(W_i * Dropout(h_i) + b_i)\n",
    "        layer_inner = self.act_fn(self.fcs[0](self.dropout(attr_matrix)))\n",
    "        for fc in self.fcs[1:-1]:\n",
    "            layer_inner = self.act_fn(fc(layer_inner))\n",
    "        res = self.fcs[-1](self.dropout(layer_inner))\n",
    "        return res\n",
    "\n",
    "    def forward(self, attr_matrix: torch.sparse.FloatTensor, idx: torch.LongTensor):\n",
    "        local_logits = self._transform_features(attr_matrix) # forward local_logits of layer h_{i+1}.\n",
    "        # using PPR propagation if necessary.\n",
    "        if self.propagation is not None:\n",
    "            final_logits = self.propagation(local_logits, idx)\n",
    "        else:\n",
    "            final_logits = local_logits[idx]\n",
    "        return F.log_softmax(final_logits, dim=-1) # to classification ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementation of EarlyStop Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopVariable(Enum): # criteria of early-stop to monitor during training\n",
    "    LOSS = auto() # according loss\n",
    "    ACCURACY = auto() # according acc\n",
    "    NONE = auto() # nothing\n",
    "\n",
    "\n",
    "class Best(Enum): # the standard to remember the best epoch.\n",
    "    RANKED = auto() # when a specific criterion improves.\n",
    "    ALL = auto() # only when all monitored criteria improve.\n",
    "\n",
    "\n",
    "stopping_args = dict(\n",
    "        stop_varnames=[StopVariable.ACCURACY, StopVariable.LOSS],\n",
    "        patience=100, max_epochs=10000, remember=Best.RANKED)\n",
    "# patience: the num of epochs to wait without improvement before stopping.\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(\n",
    "            self, model: Module, stop_varnames: List[StopVariable],\n",
    "            patience: int = 10, max_epochs: int = 200, remember: Best = Best.ALL):\n",
    "        self.model = model # to save the best model.\n",
    "        self.comp_ops = []\n",
    "        self.stop_vars = []\n",
    "        self.best_vals = []\n",
    "        for stop_varname in stop_varnames:\n",
    "            # traverse the early-stop criteria and initialize it.\n",
    "            if stop_varname is StopVariable.LOSS:\n",
    "                self.stop_vars.append('loss')\n",
    "                self.comp_ops.append(operator.le) # le: <=\n",
    "                self.best_vals.append(np.inf) # loss_initialized = inf\n",
    "            elif stop_varname is StopVariable.ACCURACY:\n",
    "                self.stop_vars.append('acc')\n",
    "                self.comp_ops.append(operator.ge) # ge: >=\n",
    "                self.best_vals.append(-np.inf) # acc_initialized = -inf\n",
    "        self.remember = remember\n",
    "        self.remembered_vals = copy.copy(self.best_vals)\n",
    "        self.max_patience = patience\n",
    "        self.patience = self.max_patience\n",
    "        self.max_epochs = max_epochs\n",
    "        self.best_epoch = None\n",
    "        self.best_state = None\n",
    "\n",
    "    # Check if has improvement.\n",
    "    def check(self, values: List[np.floating], epoch: int) -> bool:\n",
    "        checks = [self.comp_ops[i](val, self.best_vals[i])\n",
    "                  for i, val in enumerate(values)]\n",
    "        if any(checks):\n",
    "            self.best_vals = np.choose(checks, [self.best_vals, values])\n",
    "            self.patience = self.max_patience # reset patience epochs if has some improvement.\n",
    "\n",
    "            comp_remembered = [\n",
    "                    self.comp_ops[i](val, self.remembered_vals[i])\n",
    "                    for i, val in enumerate(values)]\n",
    "            if self.remember is Best.ALL: # all criteria\n",
    "                if all(comp_remembered):\n",
    "                    self.best_epoch = epoch # remember this epoch\n",
    "                    self.remembered_vals = copy.copy(values) # and its vals\n",
    "                    self.best_state = {\n",
    "                            key: value.cpu() for key, value\n",
    "                            in self.model.state_dict().items()} # and states.\n",
    "            elif self.remember is Best.RANKED: # any one criteria\n",
    "                for i, comp in enumerate(comp_remembered):\n",
    "                    if comp:\n",
    "                        if not(self.remembered_vals[i] == values[i]):\n",
    "                            self.best_epoch = epoch # remember this epoch\n",
    "                            self.remembered_vals = copy.copy(values) # and its vals\n",
    "                            self.best_state = {\n",
    "                                    key: value.cpu() for key, value\n",
    "                                    in self.model.state_dict().items()} # and states.\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "        else:\n",
    "            self.patience -= 1 # patience epochs - 1 if has no improvement.\n",
    "        return self.patience == 0 # if patience == 0 then early-stop.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The functions to split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the index that exist in idx_exclude_list from idx.\n",
    "def exclude_idx(idx: np.ndarray, idx_exclude_list: List[np.ndarray]) -> np.ndarray:\n",
    "    idx_exclude = np.concatenate(idx_exclude_list)\n",
    "    return np.array([i for i in idx if i not in idx_exclude])\n",
    "\n",
    "\n",
    "# split the index of the dataset to 2 parts(know and unknow) for semi-supervised learning task,\n",
    "# in this project, we wanna split the train-set, val-set and early-stop-set.\n",
    "def known_unknown_split(\n",
    "        idx: np.ndarray, nknown: int = 1500, seed: int = 3407) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # cite: Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    # random choose nknwn index from idx, without repeat.\n",
    "    known_idx = rnd_state.choice(idx, nknown, replace=False)\n",
    "    # exclude known idx, guarantee the choice has no repeative.\n",
    "    unknown_idx = exclude_idx(idx, [known_idx])\n",
    "    return known_idx, unknown_idx\n",
    "\n",
    "# choose train samples and early-stop samples.\n",
    "def train_stopping_split(\n",
    "        idx: np.ndarray, labels: np.ndarray, ntrain_per_class: int = 20,\n",
    "        nstopping: int = 500, seed: int = 3407) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # cite: Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    train_idx_split = []\n",
    "    for i in range(max(labels) + 1):\n",
    "        train_idx_split.append(rnd_state.choice(\n",
    "                idx[labels == i], ntrain_per_class, replace=False))\n",
    "    train_idx = np.concatenate(train_idx_split)\n",
    "    stopping_idx = rnd_state.choice(\n",
    "            exclude_idx(idx, [train_idx]),\n",
    "            nstopping, replace=False)\n",
    "    return train_idx, stopping_idx\n",
    "\n",
    "# generate the index of train-set, val-set and early-stop-set.\n",
    "def gen_splits(\n",
    "        labels: np.ndarray, idx_split_args: Dict[str, int],\n",
    "        test: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    all_idx = np.arange(len(labels))\n",
    "    known_idx, unknown_idx = known_unknown_split(\n",
    "            all_idx, idx_split_args['nknown'])\n",
    "    _, cnts = np.unique(labels[known_idx], return_counts=True)\n",
    "    stopping_split_args = copy.copy(idx_split_args)\n",
    "    del stopping_split_args['nknown']\n",
    "    train_idx, stopping_idx = train_stopping_split(\n",
    "            known_idx, labels[known_idx], **stopping_split_args)\n",
    "    if test:\n",
    "        val_idx = unknown_idx\n",
    "    else:\n",
    "        val_idx = exclude_idx(known_idx, [train_idx, stopping_idx])\n",
    "    return train_idx, stopping_idx, val_idx\n",
    "\n",
    "# normalization of attributes matrix.\n",
    "def normalize_attributes(attr_matrix):\n",
    "    epsilon = 1e-12 # avoid the error of division by 0.\n",
    "    if isinstance(attr_matrix, sp.csr_matrix):\n",
    "        attr_norms = spla.norm(attr_matrix, ord=1, axis=1)\n",
    "        attr_invnorms = 1 / np.maximum(attr_norms, epsilon)\n",
    "        attr_mat_norm = attr_matrix.multiply(attr_invnorms[:, np.newaxis])\n",
    "    else:\n",
    "        attr_norms = np.linalg.norm(attr_matrix, ord=1, axis=1)\n",
    "        attr_invnorms = 1 / np.maximum(attr_norms, epsilon)\n",
    "        attr_mat_norm = attr_matrix * attr_invnorms[:, np.newaxis]\n",
    "    return attr_mat_norm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achieve some basic Graphic Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_graph_properties = [\n",
    "        'adj_matrix', 'attr_matrix', 'labels',\n",
    "        'node_names', 'attr_names', 'class_names']\n",
    "\n",
    "\n",
    "class SparseGraph:\n",
    "    # Attributed labeled graph stored in sparse matrix form.\n",
    "    def __init__(\n",
    "            self, adj_matrix: sp.spmatrix,\n",
    "            attr_matrix: Union[np.ndarray, sp.spmatrix] = None,\n",
    "            labels: Union[np.ndarray, sp.spmatrix] = None,\n",
    "            node_names: np.ndarray = None,\n",
    "            attr_names: np.ndarray = None,\n",
    "            class_names: np.ndarray = None):\n",
    "\n",
    "        self.adj_matrix = adj_matrix # adjacent matrix of graph.\n",
    "        self.attr_matrix = attr_matrix # attributions matrix of each nodes.\n",
    "        self.labels = labels # classification of each nodes.\n",
    "        self.node_names = node_names\n",
    "        self.attr_names = attr_names\n",
    "        self.class_names = class_names\n",
    "    # Return a Numpy Array that maps edgeids to the indices in the adjacency matrix.\n",
    "    def get_edgeid_to_idx_array(self) -> np.ndarray:\n",
    "        return np.transpose(self.adj_matrix.nonzero())\n",
    "    \n",
    "    # Check if the graph adjacency matrix is not symmetric.\n",
    "    def is_directed(self) -> bool: \n",
    "        return (self.adj_matrix != self.adj_matrix.T).sum() != 0\n",
    "\n",
    "    # make adjacency matrix symmetric.\n",
    "    def to_undirected(self) -> 'SparseGraph': \n",
    "        idx = self.get_edgeid_to_idx_array().T\n",
    "        ridx = np.ravel_multi_index(idx, self.adj_matrix.shape)\n",
    "        ridx_rev = np.ravel_multi_index(idx[::-1], self.adj_matrix.shape)\n",
    "\n",
    "        # Get self-loops and opposing edges\n",
    "        dup_ridx = ridx[np.isin(ridx, ridx_rev)]\n",
    "        dup_idx = np.unravel_index(dup_ridx, self.adj_matrix.shape)\n",
    "\n",
    "        # Create symmetric matrix\n",
    "        new_adj_matrix = self.adj_matrix + self.adj_matrix.T\n",
    "        if len(dup_ridx) > 0:\n",
    "            new_adj_matrix[dup_idx] = (new_adj_matrix[dup_idx] - self.adj_matrix[dup_idx]).A1\n",
    "\n",
    "        self.adj_matrix = new_adj_matrix\n",
    "        return self\n",
    "\n",
    "    # Check if the graph edge weights other than 1.\n",
    "    def is_weighted(self) -> bool: \n",
    "        return np.any(np.unique(self.adj_matrix[self.adj_matrix.nonzero()].A1) != 1)\n",
    "\n",
    "    # set all edge weights to 1.\n",
    "    def to_unweighted(self) -> 'SparseGraph': \n",
    "        self.adj_matrix.data = np.ones_like(self.adj_matrix.data)\n",
    "        return self\n",
    "    \n",
    "    # Check if the graph is connected.\n",
    "    def is_connected(self) -> bool: \n",
    "        return sp.csgraph.connected_components(self.adj_matrix, return_labels=False) == 1\n",
    "\n",
    "    # Check if the graph has self-loops.\n",
    "    def has_self_loops(self) -> bool: \n",
    "        return not np.allclose(self.adj_matrix.diagonal(), 0)\n",
    "\n",
    "    # Perform common preprocessing steps: remove self-loops, make unweighted/undirected, select LCC(Largest Connected Components).\n",
    "    def standardize( \n",
    "            self, make_unweighted: bool = True,\n",
    "            make_undirected: bool = True,\n",
    "            no_self_loops: bool = True,\n",
    "            select_lcc: bool = True\n",
    "            ) -> 'SparseGraph':\n",
    "        G = self\n",
    "        if make_unweighted and G.is_weighted():\n",
    "            G = G.to_unweighted()\n",
    "        if make_undirected and G.is_directed():\n",
    "            G = G.to_undirected()\n",
    "        if no_self_loops and G.has_self_loops():\n",
    "            G = remove_self_loops(G)\n",
    "        if select_lcc and not G.is_connected():\n",
    "            G = largest_connected_components(G, 1)\n",
    "        return G\n",
    "\n",
    "    # Return the (A, X, E, z) quadruplet.\n",
    "    def unpack(self) -> Tuple[sp.csr_matrix,\n",
    "                              Union[np.ndarray, sp.csr_matrix],\n",
    "                              Union[np.ndarray, sp.csr_matrix]]:\n",
    "        return self.adj_matrix, self.attr_matrix, self.labels\n",
    "\n",
    "    # Return flat dictionary containing all SparseGraph properties.\n",
    "    def to_flat_dict(self) -> Dict[str, Any]:\n",
    "        data_dict = {}\n",
    "        for key in sparse_graph_properties:\n",
    "            val = getattr(self, key)\n",
    "            if sp.isspmatrix(val):\n",
    "                data_dict['{}.data'.format(key)] = val.data\n",
    "                data_dict['{}.indices'.format(key)] = val.indices\n",
    "                data_dict['{}.indptr'.format(key)] = val.indptr\n",
    "                data_dict['{}.shape'.format(key)] = val.shape\n",
    "            else:\n",
    "                data_dict[key] = val\n",
    "        return data_dict\n",
    "\n",
    "    # Initialize SparseGraph from a flat dictionary.\n",
    "    @staticmethod\n",
    "    def from_flat_dict(data_dict: Dict[str, Any]) -> 'SparseGraph':\n",
    "        init_dict = {}\n",
    "        del_entries = []\n",
    "\n",
    "        # Construct sparse matrices\n",
    "        for key in data_dict.keys():\n",
    "            if key.endswith('_data') or key.endswith('.data'):\n",
    "                if key.endswith('_data'):\n",
    "                    sep = '_'\n",
    "                else:\n",
    "                    sep = '.'\n",
    "                matrix_name = key[:-5]\n",
    "                mat_data = key\n",
    "                mat_indices = '{}{}indices'.format(matrix_name, sep)\n",
    "                mat_indptr = '{}{}indptr'.format(matrix_name, sep)\n",
    "                mat_shape = '{}{}shape'.format(matrix_name, sep)\n",
    "                if matrix_name == 'adj' or matrix_name == 'attr':\n",
    "                    matrix_name += '_matrix'\n",
    "                init_dict[matrix_name] = sp.csr_matrix(\n",
    "                        (data_dict[mat_data],\n",
    "                         data_dict[mat_indices],\n",
    "                         data_dict[mat_indptr]),\n",
    "                        shape=data_dict[mat_shape])\n",
    "                del_entries.extend([mat_data, mat_indices, mat_indptr, mat_shape])\n",
    "\n",
    "        # Delete sparse matrix entries\n",
    "        for del_entry in del_entries:\n",
    "            del data_dict[del_entry]\n",
    "\n",
    "        # Load everything else\n",
    "        for key, val in data_dict.items():\n",
    "            if ((val is not None) and (None not in val)):\n",
    "                init_dict[key] = val\n",
    "\n",
    "        # Check if the dictionary contains only entries in sparse_graph_properties\n",
    "        unknown_keys = [key for key in init_dict.keys() if key not in sparse_graph_properties]\n",
    "\n",
    "        return SparseGraph(**init_dict)\n",
    "\n",
    "# Create a graph with the specified subset of nodes.\n",
    "def create_subgraph(\n",
    "        sparse_graph: SparseGraph,\n",
    "        _sentinel: None = None,\n",
    "        nodes_to_remove: np.ndarray = None,\n",
    "        nodes_to_keep: np.ndarray = None\n",
    "        ) -> SparseGraph:\n",
    "\n",
    "    sparse_graph.adj_matrix = sparse_graph.adj_matrix[nodes_to_keep][:, nodes_to_keep]\n",
    "    if sparse_graph.attr_matrix is not None:\n",
    "        sparse_graph.attr_matrix = sparse_graph.attr_matrix[nodes_to_keep]\n",
    "    if sparse_graph.labels is not None:\n",
    "        sparse_graph.labels = sparse_graph.labels[nodes_to_keep]\n",
    "    if sparse_graph.node_names is not None:\n",
    "        sparse_graph.node_names = sparse_graph.node_names[nodes_to_keep]\n",
    "    return sparse_graph\n",
    "\n",
    "# Select the LCC(Largest Connected Components) in the graph.\n",
    "def largest_connected_components(sparse_graph: SparseGraph, n_components: int = 1) -> SparseGraph:\n",
    "    _, component_indices = sp.csgraph.connected_components(sparse_graph.adj_matrix)\n",
    "    component_sizes = np.bincount(component_indices)\n",
    "    components_to_keep = np.argsort(component_sizes)[::-1][:n_components]  # reverse order to sort descending\n",
    "    nodes_to_keep = [\n",
    "        idx for (idx, component) in enumerate(component_indices) if component in components_to_keep\n",
    "    ]\n",
    "    return create_subgraph(sparse_graph, nodes_to_keep=nodes_to_keep)\n",
    "\n",
    "# Remove self-loops\n",
    "def remove_self_loops(sparse_graph: SparseGraph) -> SparseGraph:\n",
    "    num_self_loops = (~np.isclose(sparse_graph.adj_matrix.diagonal(), 0)).sum()\n",
    "    if num_self_loops > 0:\n",
    "        sparse_graph.adj_matrix = sparse_graph.adj_matrix.tolil()\n",
    "        sparse_graph.adj_matrix.setdiag(0)\n",
    "        sparse_graph.adj_matrix = sparse_graph.adj_matrix.tocsr()\n",
    "\n",
    "    return sparse_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# packaging model training and evaluating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According the index and label of nodes, generate different PyTorch DataLoader objects for each different phases.\n",
    "def get_dataloaders(idx, labels_np, batch_size=None):\n",
    "    labels = torch.LongTensor(labels_np) # transform labels to LongTensor.\n",
    "    if batch_size is None:\n",
    "        batch_size = max((val.numel() for val in idx.values()))\n",
    "    datasets = {phase: TensorDataset(ind, labels[ind]) for phase, ind in idx.items()}\n",
    "    dataloaders = {phase: DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "                   for phase, dataset in datasets.items()}\n",
    "    return dataloaders # return Dicts of Dataloader objects of each phases.\n",
    "\n",
    "# Train a GNN(Graph Neural Network).\n",
    "def train_model(\n",
    "        name: str, model_class: Type[nn.Module], graph: SparseGraph, model_args: dict,\n",
    "        learning_rate: float, reg_lambda: float,\n",
    "        idx_split_args: dict = {'ntrain_per_class': 20, 'nstopping': 500, 'nknown': 1500, 'seed': 3407},\n",
    "        stopping_args: dict = stopping_args,\n",
    "        test: bool = False, device: str = 'cuda',\n",
    "        torch_seed: int = 3407, print_interval: int = 10) -> Tuple[nn.Module, dict, List, List]:\n",
    "    # cite: Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision\n",
    "    \n",
    "    # graph: is a Sparse graph, include adjacent matrix, node attribute matrix, labels, etc.\n",
    "    labels_all = graph.labels\n",
    "    idx_np = {}\n",
    "    # split the train-set, val-set and early-stop-set, and return their index.\n",
    "    idx_np['train'], idx_np['stopping'], idx_np['valtest'] = gen_splits(\n",
    "            labels_all, idx_split_args, test=test)\n",
    "    # Then, transform them to LongTensor\n",
    "    idx_all = {key: torch.LongTensor(val) for key, val in idx_np.items()}\n",
    "\n",
    "    logging.log(21, f\"{model_class.__name__}: {model_args}\")\n",
    "    torch.manual_seed(seed=torch_seed)\n",
    "    logging.log(22, f\"PyTorch seed: {torch_seed}\")\n",
    "\n",
    "    nfeatures = graph.attr_matrix.shape[1]\n",
    "    nclasses = max(labels_all) + 1\n",
    "    model = model_class(nfeatures, nclasses, **model_args).to(device)\n",
    "\n",
    "    reg_lambda = torch.tensor(reg_lambda, device=device) # to control the strength of the L2 regularization.\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # using Adam optimizer.\n",
    "\n",
    "    dataloaders = get_dataloaders(idx_all, labels_all) # generate DataLoaders of train-set, val-set and early-stop-set.\n",
    "    early_stopping = EarlyStopping(model, **stopping_args) # Initialize early-stop policy.\n",
    "    attr_mat_norm_np = normalize_attributes(graph.attr_matrix) # normalization and,\n",
    "    attr_mat_norm = matrix_to_torch(attr_mat_norm_np).to(device) # put it in cuda if it's available.\n",
    "\n",
    "    epoch_stats = {'train': {}, 'stopping': {}}\n",
    "\n",
    "    losses = []\n",
    "    acces = []\n",
    "    for epoch in range(early_stopping.max_epochs):\n",
    "        for phase in epoch_stats.keys():\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for idx, labels in dataloaders[phase]: # loading batched datas via dataloader of current phase.\n",
    "                idx = idx.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    log_preds = model(attr_mat_norm, idx)\n",
    "                    preds = torch.argmax(log_preds, dim=1)\n",
    "\n",
    "                    # Calculate lnegative log likelihood loss.\n",
    "                    cross_entropy_mean = F.nll_loss(log_preds, labels)\n",
    "                    # regularization.\n",
    "                    l2_reg = sum((torch.sum(param ** 2) for param in model.reg_params))\n",
    "                    loss = cross_entropy_mean + reg_lambda / 2 * l2_reg\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # collect loss and acc\n",
    "                    running_loss += loss.item() * idx.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "            # compute loss and acc.\n",
    "            epoch_stats[phase]['loss'] = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_stats[phase]['acc'] = running_corrects.item() / len(dataloaders[phase].dataset)\n",
    "\n",
    "        # record loss and acc. additionally, check early-stop.\n",
    "        if epoch % print_interval == 0:\n",
    "            logging.info(f\"Epoch {epoch}: \"\n",
    "                         f\"Train loss = {epoch_stats['train']['loss']:.2f}, \"\n",
    "                         f\"train acc = {epoch_stats['train']['acc'] * 100:.1f}, \"\n",
    "                         f\"early stopping loss = {epoch_stats['stopping']['loss']:.2f}, \"\n",
    "                         f\"early stopping acc = {epoch_stats['stopping']['acc'] * 100:.1f} \")\n",
    "            a = epoch_stats['train']['loss']\n",
    "            b = epoch_stats['train']['acc'] * 100\n",
    "            losses.append(a)\n",
    "            acces.append(b)\n",
    "\n",
    "        # check early-stop.\n",
    "        if len(early_stopping.stop_vars) > 0:\n",
    "            stop_vars = [epoch_stats['stopping'][key]\n",
    "                         for key in early_stopping.stop_vars]\n",
    "            if early_stopping.check(stop_vars, epoch):\n",
    "                break\n",
    "    logging.log(22, f\"Last epoch: {epoch}, best epoch: {early_stopping.best_epoch}\")\n",
    "\n",
    "    # Load best model weights, and then test model.\n",
    "    model.load_state_dict(early_stopping.best_state)\n",
    "\n",
    "    train_preds = get_predictions(model, attr_mat_norm, idx_all['train'])\n",
    "    train_acc = (train_preds == labels_all[idx_all['train']]).mean()\n",
    "\n",
    "    stopping_preds = get_predictions(model, attr_mat_norm, idx_all['stopping'])\n",
    "    stopping_acc = (stopping_preds == labels_all[idx_all['stopping']]).mean()\n",
    "    logging.log(21, f\"Early stopping accuracy: {stopping_acc * 100:.1f}%\")\n",
    "\n",
    "    valtest_preds = get_predictions(model, attr_mat_norm, idx_all['valtest'])\n",
    "    # print(valtest_preds)\n",
    "    valtest_acc = (valtest_preds == labels_all[idx_all['valtest']]).mean()\n",
    "    valtest_name = 'Test' if test else 'Validation'\n",
    "    logging.log(22, f\"{valtest_name} accuracy: {valtest_acc * 100:.1f}%\")\n",
    "    \n",
    "    result = {}\n",
    "    result['predictions'] = get_predictions(model, attr_mat_norm, torch.arange(len(labels_all)))\n",
    "    result['train'] = {'accuracy': train_acc}\n",
    "    result['early_stopping'] = {'accuracy': stopping_acc}\n",
    "    result['valtest'] = {'accuracy': valtest_acc}\n",
    "\n",
    "    return model, result, losses, acces\n",
    "\n",
    "# get predicted classification labels.\n",
    "def get_predictions(model, attr_matrix, idx, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = idx.numel()\n",
    "    dataset = TensorDataset(idx)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "    for idx, in dataloader:\n",
    "        idx = idx.to(attr_matrix.device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            log_preds = model(attr_matrix, idx)\n",
    "            preds.append(torch.argmax(log_preds, dim=1))\n",
    "    return torch.cat(preds, dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format='%(asctime)s: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        level=logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SparseGraph at 0x1255b7f90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"./data/\" # the path of datas in your own PC.\n",
    "\n",
    "\n",
    "def load_from_npz(file_name: str) -> SparseGraph:\n",
    "    # Load a SparseGraph from a Numpy binary file.\n",
    "    with np.load(file_name, allow_pickle=True) as loader:\n",
    "        loader = dict(loader)\n",
    "        dataset = SparseGraph.from_flat_dict(loader)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_dataset(name: str,\n",
    "                 directory: Union[Path, str] = data_dir\n",
    "                 ) -> SparseGraph:\n",
    "    # Load a dataset.\n",
    "    if isinstance(directory, str):\n",
    "        directory = Path(directory)\n",
    "    if not name.endswith('.npz'):\n",
    "        name += '.npz'\n",
    "    path_to_file = directory / name\n",
    "    if path_to_file.exists():\n",
    "        return load_from_npz(path_to_file)\n",
    "    else:\n",
    "        raise ValueError(\"{} doesn't exist.\".format(path_to_file))\n",
    "\n",
    "graph_name = 'citeseer'\n",
    "graph = load_dataset(graph_name)\n",
    "# remove self-loops -> make unweighted/undirected -> select LCC(Largest Connected Components).\n",
    "graph.standardize(select_lcc=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosse a propagation algorith\n",
    "\n",
    "Compare 3 different propagation policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prop_pprgnn = PPRExact(graph.adj_matrix, alpha=0.1)\n",
    "prop_apprgnn = PPRPowerIteration(graph.adj_matrix, alpha=0.1, niter=10)\n",
    "no_prop_apprgnn = NoPPRPowerIteration(graph.adj_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args_apprgnn = {\n",
    "    'hiddenunits': [64],\n",
    "    'drop_prob': 0.5,\n",
    "    'propagation': prop_apprgnn}\n",
    "# model_args_noapprgnn = {\n",
    "#     'hiddenunits': [64],\n",
    "#     'drop_prob': 0.5,\n",
    "#     'propagation': no_prop_apprgnn\n",
    "# }\n",
    "\n",
    "# dataset split args\n",
    "idx_split_args = {'ntrain_per_class': 20, 'nstopping': 500, 'nknown': 1500, 'seed': 3407}\n",
    "# cite: Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision\n",
    "reg_lambda = 5e-3\n",
    "learning_rate = 0.01\n",
    "\n",
    "test = True\n",
    "device = 'cpu'\n",
    "print_interval = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 20:22:15: PPRGNN: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': PPRPowerIteration()}\n",
      "2024-11-07 20:22:15: PyTorch seed: 3407\n",
      "2024-11-07 20:22:15: Epoch 0: Train loss = 1.85, train acc = 12.5, early stopping loss = 1.81, early stopping acc = 52.2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using PPR propagation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 20:22:16: Epoch 20: Train loss = 1.77, train acc = 84.2, early stopping loss = 1.79, early stopping acc = 66.6 \n",
      "2024-11-07 20:22:17: Epoch 40: Train loss = 1.71, train acc = 88.3, early stopping loss = 1.75, early stopping acc = 71.2 \n",
      "2024-11-07 20:22:18: Epoch 60: Train loss = 1.60, train acc = 85.8, early stopping loss = 1.70, early stopping acc = 72.4 \n",
      "2024-11-07 20:22:18: Epoch 80: Train loss = 1.47, train acc = 88.3, early stopping loss = 1.65, early stopping acc = 72.8 \n",
      "2024-11-07 20:22:19: Epoch 100: Train loss = 1.38, train acc = 90.0, early stopping loss = 1.59, early stopping acc = 73.0 \n",
      "2024-11-07 20:22:20: Epoch 120: Train loss = 1.28, train acc = 94.2, early stopping loss = 1.55, early stopping acc = 72.2 \n",
      "2024-11-07 20:22:21: Epoch 140: Train loss = 1.23, train acc = 95.0, early stopping loss = 1.51, early stopping acc = 72.4 \n",
      "2024-11-07 20:22:21: Epoch 160: Train loss = 1.13, train acc = 94.2, early stopping loss = 1.48, early stopping acc = 72.4 \n",
      "2024-11-07 20:22:22: Epoch 180: Train loss = 1.07, train acc = 94.2, early stopping loss = 1.44, early stopping acc = 72.6 \n",
      "2024-11-07 20:22:23: Epoch 200: Train loss = 1.06, train acc = 94.2, early stopping loss = 1.42, early stopping acc = 71.8 \n",
      "2024-11-07 20:22:23: Epoch 220: Train loss = 0.98, train acc = 95.0, early stopping loss = 1.40, early stopping acc = 71.6 \n",
      "2024-11-07 20:22:24: Epoch 240: Train loss = 0.97, train acc = 93.3, early stopping loss = 1.38, early stopping acc = 71.8 \n",
      "2024-11-07 20:22:25: Epoch 260: Train loss = 0.92, train acc = 95.8, early stopping loss = 1.35, early stopping acc = 72.4 \n",
      "2024-11-07 20:22:26: Epoch 280: Train loss = 0.88, train acc = 97.5, early stopping loss = 1.35, early stopping acc = 71.6 \n",
      "2024-11-07 20:22:27: Epoch 300: Train loss = 0.87, train acc = 96.7, early stopping loss = 1.33, early stopping acc = 72.4 \n",
      "2024-11-07 20:22:27: Epoch 320: Train loss = 0.83, train acc = 98.3, early stopping loss = 1.32, early stopping acc = 72.4 \n",
      "2024-11-07 20:22:28: Epoch 340: Train loss = 0.80, train acc = 97.5, early stopping loss = 1.30, early stopping acc = 71.4 \n",
      "2024-11-07 20:22:29: Epoch 360: Train loss = 0.82, train acc = 97.5, early stopping loss = 1.30, early stopping acc = 71.8 \n",
      "2024-11-07 20:22:30: Epoch 380: Train loss = 0.77, train acc = 97.5, early stopping loss = 1.29, early stopping acc = 70.6 \n",
      "2024-11-07 20:22:30: Epoch 400: Train loss = 0.74, train acc = 98.3, early stopping loss = 1.29, early stopping acc = 70.2 \n",
      "2024-11-07 20:22:31: Epoch 420: Train loss = 0.72, train acc = 98.3, early stopping loss = 1.27, early stopping acc = 70.8 \n",
      "2024-11-07 20:22:32: Epoch 440: Train loss = 0.74, train acc = 99.2, early stopping loss = 1.26, early stopping acc = 70.8 \n",
      "2024-11-07 20:22:32: Epoch 460: Train loss = 0.68, train acc = 96.7, early stopping loss = 1.23, early stopping acc = 72.2 \n",
      "2024-11-07 20:22:33: Epoch 480: Train loss = 0.68, train acc = 95.8, early stopping loss = 1.26, early stopping acc = 71.4 \n",
      "2024-11-07 20:22:34: Epoch 500: Train loss = 0.68, train acc = 98.3, early stopping loss = 1.24, early stopping acc = 71.6 \n",
      "2024-11-07 20:22:35: Epoch 520: Train loss = 0.66, train acc = 99.2, early stopping loss = 1.24, early stopping acc = 71.4 \n",
      "2024-11-07 20:22:35: Epoch 540: Train loss = 0.63, train acc = 98.3, early stopping loss = 1.23, early stopping acc = 71.2 \n",
      "2024-11-07 20:22:36: Epoch 560: Train loss = 0.67, train acc = 95.0, early stopping loss = 1.23, early stopping acc = 71.4 \n",
      "2024-11-07 20:22:37: Epoch 580: Train loss = 0.63, train acc = 99.2, early stopping loss = 1.23, early stopping acc = 70.8 \n",
      "2024-11-07 20:22:38: Epoch 600: Train loss = 0.61, train acc = 97.5, early stopping loss = 1.23, early stopping acc = 70.8 \n",
      "2024-11-07 20:22:38: Epoch 620: Train loss = 0.61, train acc = 98.3, early stopping loss = 1.22, early stopping acc = 70.4 \n",
      "2024-11-07 20:22:39: Epoch 640: Train loss = 0.60, train acc = 99.2, early stopping loss = 1.20, early stopping acc = 71.4 \n",
      "2024-11-07 20:22:40: Epoch 660: Train loss = 0.59, train acc = 97.5, early stopping loss = 1.19, early stopping acc = 71.4 \n",
      "2024-11-07 20:22:41: Epoch 680: Train loss = 0.61, train acc = 96.7, early stopping loss = 1.18, early stopping acc = 72.2 \n",
      "2024-11-07 20:22:41: Epoch 700: Train loss = 0.58, train acc = 99.2, early stopping loss = 1.20, early stopping acc = 71.2 \n",
      "2024-11-07 20:22:42: Epoch 720: Train loss = 0.58, train acc = 98.3, early stopping loss = 1.18, early stopping acc = 71.8 \n",
      "2024-11-07 20:22:43: Epoch 740: Train loss = 0.57, train acc = 98.3, early stopping loss = 1.19, early stopping acc = 70.4 \n",
      "2024-11-07 20:22:44: Epoch 760: Train loss = 0.55, train acc = 98.3, early stopping loss = 1.20, early stopping acc = 70.6 \n",
      "2024-11-07 20:22:44: Epoch 780: Train loss = 0.58, train acc = 99.2, early stopping loss = 1.17, early stopping acc = 71.6 \n",
      "2024-11-07 20:22:45: Epoch 800: Train loss = 0.53, train acc = 98.3, early stopping loss = 1.16, early stopping acc = 72.0 \n",
      "2024-11-07 20:22:46: Epoch 820: Train loss = 0.53, train acc = 96.7, early stopping loss = 1.18, early stopping acc = 71.0 \n",
      "2024-11-07 20:22:47: Epoch 840: Train loss = 0.52, train acc = 97.5, early stopping loss = 1.17, early stopping acc = 70.8 \n",
      "2024-11-07 20:22:48: Epoch 860: Train loss = 0.51, train acc = 99.2, early stopping loss = 1.14, early stopping acc = 71.2 \n",
      "2024-11-07 20:22:48: Epoch 880: Train loss = 0.52, train acc = 100.0, early stopping loss = 1.15, early stopping acc = 71.8 \n",
      "2024-11-07 20:22:49: Epoch 900: Train loss = 0.49, train acc = 100.0, early stopping loss = 1.14, early stopping acc = 71.6 \n",
      "2024-11-07 20:22:50: Epoch 920: Train loss = 0.48, train acc = 98.3, early stopping loss = 1.15, early stopping acc = 72.0 \n",
      "2024-11-07 20:22:51: Epoch 940: Train loss = 0.48, train acc = 98.3, early stopping loss = 1.13, early stopping acc = 71.0 \n",
      "2024-11-07 20:22:51: Epoch 960: Train loss = 0.47, train acc = 97.5, early stopping loss = 1.18, early stopping acc = 70.8 \n",
      "2024-11-07 20:22:52: Epoch 980: Train loss = 0.48, train acc = 98.3, early stopping loss = 1.18, early stopping acc = 70.0 \n",
      "2024-11-07 20:22:53: Epoch 1000: Train loss = 0.47, train acc = 100.0, early stopping loss = 1.14, early stopping acc = 71.2 \n",
      "2024-11-07 20:22:53: Epoch 1020: Train loss = 0.47, train acc = 98.3, early stopping loss = 1.13, early stopping acc = 70.4 \n",
      "2024-11-07 20:22:54: Epoch 1040: Train loss = 0.49, train acc = 98.3, early stopping loss = 1.12, early stopping acc = 71.6 \n",
      "2024-11-07 20:22:55: Epoch 1060: Train loss = 0.47, train acc = 98.3, early stopping loss = 1.15, early stopping acc = 70.4 \n",
      "2024-11-07 20:22:56: Epoch 1080: Train loss = 0.47, train acc = 98.3, early stopping loss = 1.14, early stopping acc = 70.8 \n",
      "2024-11-07 20:22:56: Epoch 1100: Train loss = 0.46, train acc = 100.0, early stopping loss = 1.15, early stopping acc = 70.8 \n",
      "2024-11-07 20:22:57: Last epoch: 1118, best epoch: 166\n",
      "2024-11-07 20:22:57: Early stopping accuracy: 70.8%\n",
      "2024-11-07 20:22:57: Test accuracy: 68.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"using PPR propagation...\")\n",
    "model, result, losses_ppr, acces_ppr = train_model(\n",
    "        graph_name, PPRGNN, graph, model_args_apprgnn, learning_rate, reg_lambda,\n",
    "        idx_split_args, stopping_args, test, device, 3407, print_interval)\n",
    "\n",
    "# PPRGCN(\n",
    "\n",
    "#   (fcs): ModuleList(\n",
    "    \n",
    "#     (0): MixedLinear()\n",
    "#     (1): Linear(in_features=64, out_features=6, bias=False)\n",
    "#   )\n",
    "#   (dropout): MixedDropout(\n",
    "#     (dense_dropout): Dropout(p=0.5, inplace=False)\n",
    "#     (sparse_dropout): SparseDropout()\n",
    "#   )\n",
    "#   (act_fn): ReLU()\n",
    "#   (propagation): PPRPowerIteration()\n",
    "# )\n",
    "\n",
    "# print(\"w/o PPR propagation...\")\n",
    "# model, result, losses, acces = train_model(\n",
    "#         graph_name, PPRGNN, graph, model_args_noapprgnn, learning_rate, reg_lambda,\n",
    "#         idx_split_args, stopping_args, test, device, 3407, print_interval)\n",
    "# cite: Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision\n",
    "\n",
    "# [5 0 0 4 1 2 0 4 2 3 2 0 4 1 4 3 4 4 3 4 2 4 0 2 4 2 2 0 1 2 4 4 5 3 4 4 2\n",
    "#  4 3 0 1 2 4 0 1 3 4 1 0 1 4 4 1 4 5 2 4 5 0 0 4 3 4 4 0 5 2 4 4 0 5 1 2 1\n",
    "#  2 1 3 1 1 4 1 3 3 1 1 4 3 5 4 4 4 2 3 2 2 4 0 1 2 3 2 0 3 0 5 4 4 4 4 4 4\n",
    "#  0 5 1 2 1 1 2 1 1 1 4 0 0 4 5 4 1 1 1 4 1 4 0 2 4 4 0 4 4 4 4 4 4 0 1 4 2\n",
    "#  4 3 3 4 0 4 4 0 4 0 0 4 2 4 4 4 4 3 4 1 4 0 4 5 3 1 1 4 1 2 1 4 5 0 4 0 0\n",
    "#  0 4 2 4 2 1 4 1 2 1 4 0 0 4 4 2 2 2 3 4 4 4 1 0 1 5 5 4 4 4 2 2 1 2 3 4 3\n",
    "#  1 5 1 0 0 1 1 4 0 3 1 0 1 4 1 1 3 4 0 2 2 4 4 4 5 4 5 0 3 4 4 5 4 4 4 2 2\n",
    "#  4 4 1 0 4 3 4 5 0 4 4 4 4 4 1 5 4 4 4 4 1 3 0 4 2 5 3 5 1 4 4 4 4 4 4 3 1\n",
    "#  3 1 2 5 4 3 2 4 4 0 0 4 4 1 0 5 1 4 2 2 2 0 0 4 4 4 0 4 3 4 2 2 4 1 1 1 5\n",
    "#  4 4 4 2 4 4 3 4 2 4 4 4 1 1 2 2 0 4 4 4 3 3 0 4 0 3 3 4 3 3 4 1 1 1 4 0 4\n",
    "#  0 0 4 4 4 3 0 0 1 5 5 5 5 5 5 4 4 1 1 4 4 4 5 1 4 1 4 4 0 4 2 1 2 2 0 1 4\n",
    "#  4 3 4 4 1 3 5 5 4 2 2 3 4 0 3 1 1 4 1 1 0 0 5 3 2 5 0 1 3 1 1 0 1 3 4 1 4\n",
    "#  2 0 4 4 0 0 4 4 4 0 1 1 5 3 3 4 1 4 1 4 2 1 4 4 1 4 4 1 3 0 1 3 4 4 4 2 1\n",
    "#  1 2 2 1 4 1 3 4 3 2 3 1 3 4 4 4 4 3 0 1 1 0 5 0 5 1 4 4 3 3 3 1 1 1 4 2 1\n",
    "#  5 5 1 0 0 1 5 4 5 4 1 3 3 3 1 1 0 1 4 4 4 4 4 1 2 1 4 5 5 5 2 3 0 3 3 0 2\n",
    "#  5 4 2 5 4 4 2 3 2 1 2 0 0 3 1 1 1 2 0 0 4 1 0 3 2 0 2 5 1 1 5 0 4 4 3 4 4\n",
    "#  1 0 1 1 1 0 1 4 5 1 4 4 4 4 5 4 5 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print and display result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(losses_ppr)\n",
    "# # print(acces_ppr)\n",
    "# # print(losses)\n",
    "# # print(acces)\n",
    "# # # cora\n",
    "# # losses_y = [1.999155044555664, 1.9360873699188232, 1.8947722911834717, 1.8200101852416992, 1.7286542654037476, 1.6306400299072266, 1.539429783821106, 1.4232006072998047, 1.346359372138977, 1.2886582612991333, 1.226112961769104, 1.188503384590149, 1.1336151361465454, 1.097508192062378, 1.0523463487625122, 1.0449994802474976, 0.9971270561218262, 0.9760173559188843, 0.9320552349090576, 0.9432700872421265, 0.8877965211868286, 0.877456784248352, 0.8718149662017822, 0.8471935987472534, 0.8269876837730408, 0.8135499954223633, 0.7799643874168396, 0.7839599847793579, 0.7348724603652954, 0.7811635732650757, 0.7381916046142578, 0.7308164834976196, 0.7286688089370728, 0.7211811542510986, 0.7055493593215942, 0.6984845399856567, 0.6841812133789062, 0.6505553722381592, 0.6520329117774963, 0.6648193597793579, 0.6148969531059265, 0.6534010171890259, 0.6330472230911255, 0.6221397519111633, 0.596966028213501, 0.6052400469779968, 0.5926449298858643, 0.5779943466186523, 0.5649635791778564, 0.5799045562744141, 0.5793216228485107, 0.5506519079208374, 0.5644961595535278, 0.5638866424560547, 0.5454038977622986, 0.5173771381378174, 0.5511174201965332, 0.5231750011444092, 0.518358588218689, 0.5234871506690979, 0.5063127279281616, 0.5173208713531494, 0.5065826773643494, 0.5114019513130188, 0.48499825596809387, 0.49187952280044556, 0.481928288936615, 0.505889892578125, 0.492171049118042, 0.4636811912059784, 0.4770529866218567, 0.4954625964164734, 0.48253771662712097, 0.45510801672935486, 0.4841533899307251, 0.4828181862831116, 0.46311426162719727, 0.45584261417388916, 0.4535939395427704, 0.45560747385025024, 0.45006388425827026]\n",
    "# # acces_y = [13.571428571428571, 49.28571428571429, 54.285714285714285, 74.28571428571429, 80.0, 87.85714285714286, 92.85714285714286, 91.42857142857143, 92.85714285714286, 93.57142857142857, 93.57142857142857, 95.71428571428572, 97.85714285714285, 97.14285714285714, 96.42857142857143, 94.28571428571428, 97.14285714285714, 95.71428571428572, 97.14285714285714, 96.42857142857143, 96.42857142857143, 96.42857142857143, 99.28571428571429, 97.85714285714285, 95.71428571428572, 97.85714285714285, 98.57142857142858, 96.42857142857143, 98.57142857142858, 98.57142857142858, 98.57142857142858, 97.85714285714285, 98.57142857142858, 98.57142857142858, 97.85714285714285, 99.28571428571429, 97.14285714285714, 97.85714285714285, 99.28571428571429, 99.28571428571429, 97.85714285714285, 97.14285714285714, 100.0, 100.0, 100.0, 98.57142857142858, 98.57142857142858, 99.28571428571429, 100.0, 97.85714285714285, 99.28571428571429, 99.28571428571429, 99.28571428571429, 98.57142857142858, 99.28571428571429, 100.0, 97.85714285714285, 100.0, 99.28571428571429, 99.28571428571429, 98.57142857142858, 99.28571428571429, 100.0, 99.28571428571429, 97.85714285714285, 99.28571428571429, 100.0, 100.0, 99.28571428571429, 100.0, 100.0, 99.28571428571429, 99.28571428571429, 100.0, 100.0, 99.28571428571429, 98.57142857142858, 99.28571428571429, 99.28571428571429, 98.57142857142858, 100.0]\n",
    "# # # 1600 80.0%\n",
    "\n",
    "# # losses_n = [1.9991661310195923, 1.9250173568725586, 1.8503679037094116, 1.7535756826400757, 1.6060858964920044, 1.471642017364502, 1.36625075340271, 1.2836620807647705, 1.1945551633834839, 1.1071583032608032, 1.0631866455078125, 1.0076262950897217, 1.0120394229888916, 0.9284894466400146, 0.8736308813095093, 0.8701555132865906, 0.862390398979187, 0.8338152766227722, 0.8019628524780273, 0.7609272003173828, 0.7721467018127441, 0.7513244152069092, 0.7084451913833618, 0.7090206146240234, 0.7075529098510742, 0.6869310736656189, 0.6649386882781982, 0.634772777557373, 0.6190961003303528, 0.6273723840713501, 0.6125494241714478, 0.625257134437561, 0.5822364091873169, 0.5777968764305115, 0.553878664970398, 0.5325292348861694, 0.5536878108978271, 0.5429221391677856, 0.535572350025177, 0.5404436588287354, 0.5259439945220947, 0.5303497910499573, 0.5379989147186279, 0.4946139454841614, 0.5123113393783569, 0.4921264052391052, 0.48965752124786377, 0.4848070740699768, 0.4778388738632202, 0.48580652475357056, 0.47602754831314087, 0.47778505086898804, 0.4677654206752777, 0.4452298879623413, 0.44782257080078125, 0.45850950479507446, 0.4494389295578003, 0.4311066269874573, 0.4408494830131531, 0.4302259683609009, 0.41433584690093994, 0.4277414381504059, 0.4154156446456909, 0.4200807809829712, 0.41554853320121765, 0.3913927376270294, 0.43172046542167664, 0.40822237730026245]\n",
    "# # acces_n = [22.857142857142858, 64.28571428571429, 75.71428571428571, 88.57142857142857, 91.42857142857143, 97.14285714285714, 97.85714285714285, 97.85714285714285, 98.57142857142858, 99.28571428571429, 98.57142857142858, 98.57142857142858, 97.85714285714285, 100.0, 100.0, 100.0, 99.28571428571429, 100.0, 99.28571428571429, 100.0, 99.28571428571429, 99.28571428571429, 100.0, 100.0, 99.28571428571429, 100.0, 100.0, 99.28571428571429, 100.0, 100.0, 98.57142857142858, 97.85714285714285, 100.0, 100.0, 99.28571428571429, 100.0, 99.28571428571429, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.28571428571429, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.28571428571429, 100.0, 100.0, 100.0, 100.0, 98.57142857142858, 99.28571428571429, 100.0, 100.0, 100.0]\n",
    "# # # 1340 83.4%\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# losses_n = [1.8450037240982056, 1.7636486291885376, 1.681015133857727, 1.562037706375122, 1.391516923904419, 1.265153408050537, 1.1776729822158813, 1.101589560508728, 1.0096275806427002, 0.9296137094497681, 0.8773857355117798, 0.8562635779380798, 0.8320132493972778, 0.814453661441803, 0.7568849325180054, 0.7779812812805176, 0.7150870561599731, 0.712223470211029, 0.7008998990058899, 0.6723612546920776, 0.6568748354911804, 0.6373711824417114, 0.5883890390396118, 0.6062990427017212, 0.5902689099311829, 0.5702847242355347, 0.5455150008201599, 0.5613892674446106, 0.5450415015220642, 0.541067898273468, 0.5403109788894653, 0.5470931529998779, 0.523347020149231, 0.4888240098953247, 0.529666543006897, 0.5292468070983887, 0.5063456892967224, 0.5066936612129211, 0.5053332448005676, 0.48861581087112427, 0.44873151183128357, 0.4674129784107208, 0.4512367248535156, 0.4655795097351074, 0.43297815322875977, 0.43294477462768555, 0.4345824718475342]\n",
    "# acces_n = [18.333333333333332, 90.83333333333333, 87.5, 95.83333333333334, 95.0, 92.5, 96.66666666666667, 95.0, 98.33333333333333, 97.5, 96.66666666666667, 99.16666666666667, 98.33333333333333, 97.5, 99.16666666666667, 98.33333333333333, 99.16666666666667, 98.33333333333333, 100.0, 99.16666666666667, 97.5, 97.5, 100.0, 99.16666666666667, 98.33333333333333, 98.33333333333333, 99.16666666666667, 98.33333333333333, 100.0, 98.33333333333333, 97.5, 98.33333333333333, 98.33333333333333, 99.16666666666667, 96.66666666666667, 98.33333333333333, 100.0, 99.16666666666667, 97.5, 98.33333333333333, 100.0, 99.16666666666667, 98.33333333333333, 99.16666666666667, 99.16666666666667, 99.16666666666667, 100.0]\n",
    "# # 920 69.3%\n",
    "\n",
    "# losses_y = [1.8450497388839722, 1.7721025943756104, 1.7075320482254028, 1.6069467067718506, 1.4969310760498047, 1.3683867454528809, 1.2692360877990723, 1.170819640159607, 1.1004536151885986, 1.0775530338287354, 1.0203838348388672, 0.9840044975280762, 0.9483926296234131, 0.9116121530532837, 0.8749239444732666, 0.8418118357658386, 0.8225420117378235, 0.7783620953559875, 0.7855359315872192, 0.7450823783874512, 0.7414303421974182, 0.7536789178848267, 0.6979372501373291, 0.7214009761810303, 0.6815779805183411, 0.657567024230957, 0.690597653388977, 0.6351172924041748, 0.6206866502761841, 0.6004180908203125, 0.6246891021728516, 0.6226232647895813, 0.5789870023727417, 0.6306267976760864, 0.5648950338363647, 0.5743144750595093, 0.5654351711273193, 0.5672489404678345, 0.5585963726043701, 0.5438681840896606, 0.5335774421691895, 0.5410810112953186]\n",
    "# acces_y = [23.333333333333332, 77.5, 80.0, 87.5, 85.83333333333333, 90.0, 89.16666666666667, 91.66666666666666, 91.66666666666666, 90.0, 91.66666666666666, 94.16666666666667, 91.66666666666666, 95.0, 94.16666666666667, 92.5, 95.0, 95.0, 95.0, 96.66666666666667, 95.0, 93.33333333333333, 95.0, 95.0, 95.83333333333334, 95.83333333333334, 95.83333333333334, 95.0, 95.83333333333334, 98.33333333333333, 96.66666666666667, 95.0, 95.83333333333334, 95.83333333333334, 97.5, 99.16666666666667, 97.5, 95.83333333333334, 98.33333333333333, 95.83333333333334, 95.83333333333334, 95.0]\n",
    "# # 820 72.8%\n",
    "\n",
    "# epochs_y = np.linspace(0, 820, len(losses_y))\n",
    "# epochs_n = np.linspace(0, 920, len(losses_n))\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# ax1.plot(epochs_y, losses_y,linestyle='--', label=\"+ PageRank\", color=\"blue\")\n",
    "# ax1.plot(epochs_n, losses_n,linestyle='--', label=\"w/o PageRank\", color=\"orange\")\n",
    "\n",
    "# ax1.set_ylabel('Loss')\n",
    "# ax1.set_ylim([0, 2.0])\n",
    "# ax1.set_xlabel('Epochs')\n",
    "# ax1.grid(True)\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "\n",
    "# ax2.plot(epochs_y, acces_y, label=\"+ PageRank\", color=\"blue\")\n",
    "# ax2.plot(epochs_n, acces_n, label=\"w/o PageRank\", color=\"orange\")\n",
    "# ax2.set_ylabel('Accuracy')\n",
    "# ax2.axhline(y=72.8, color='blue', linewidth=3, linestyle='-', )\n",
    "# ax2.axhline(y=69.3, color='orange', linewidth=3, linestyle='-')\n",
    "\n",
    "# ax2.text(0.7, 0.67, '72.8%', color='red', fontsize=12, verticalalignment='center', horizontalalignment='left', transform=ax2.transAxes)\n",
    "# ax2.text(0.7, 0.592, '69.3%', color='red', fontsize=12, verticalalignment='center', horizontalalignment='left', transform=ax2.transAxes)\n",
    "\n",
    "# ax2.legend(loc=\"upper right\", bbox_to_anchor=(1, 0.9)) \n",
    "\n",
    "# plt.title('Loss and Accuracy of Dataset CITESEER')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./cora.png\" alt=\"ç¤ºä¾‹å›¾ç‰‡\" width=\"570\"> <img src=\"./CITESEER.png\" alt=\"ç¤ºä¾‹å›¾ç‰‡\" width=\"575\">\n",
    "\n",
    "<img src=\"./K.png\" alt=\"ç¤ºä¾‹å›¾ç‰‡\" width=\"1150\">\n",
    "\n",
    "<img src=\"./alpha.png\" alt=\"ç¤ºä¾‹å›¾ç‰‡\" width=\"1150\">\n",
    "\n",
    "<img src=\"./display.png\" alt=\"ç¤ºä¾‹å›¾ç‰‡\" width=\"500\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison: A example implement of K-means cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 21:15:27.098111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      paperid  w0  w1  w2  w3  w4  w5  w6  w7  w8  ...  w1424  w1425  w1426  \\\n",
      "0       31336   0   0   0   0   0   0   0   0   0  ...      0      0      1   \n",
      "1     1061127   0   0   0   0   0   0   0   0   0  ...      0      1      0   \n",
      "2     1106406   0   0   0   0   0   0   0   0   0  ...      0      0      0   \n",
      "3       13195   0   0   0   0   0   0   0   0   0  ...      0      0      0   \n",
      "4       37879   0   0   0   0   0   0   0   0   0  ...      0      0      0   \n",
      "...       ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...    ...    ...    ...   \n",
      "2703  1128975   0   0   0   0   0   0   0   0   0  ...      0      0      0   \n",
      "2704  1128977   0   0   0   0   0   0   0   0   0  ...      0      0      0   \n",
      "2705  1128978   0   0   0   0   0   0   0   0   0  ...      0      0      0   \n",
      "2706   117328   0   0   0   0   1   0   0   0   0  ...      0      0      0   \n",
      "2707    24043   0   0   0   0   0   0   0   0   0  ...      0      0      0   \n",
      "\n",
      "      w1427  w1428  w1429  w1430  w1431  w1432                   label  \n",
      "0         0      0      0      0      0      0         Neural_Networks  \n",
      "1         0      0      0      0      0      0           Rule_Learning  \n",
      "2         0      0      0      0      0      0  Reinforcement_Learning  \n",
      "3         0      0      0      0      0      0  Reinforcement_Learning  \n",
      "4         0      0      0      0      0      0   Probabilistic_Methods  \n",
      "...     ...    ...    ...    ...    ...    ...                     ...  \n",
      "2703      0      0      0      0      0      0      Genetic_Algorithms  \n",
      "2704      0      0      0      0      0      0      Genetic_Algorithms  \n",
      "2705      0      0      0      0      0      0      Genetic_Algorithms  \n",
      "2706      0      0      0      0      0      0              Case_Based  \n",
      "2707      0      0      0      0      0      0         Neural_Networks  \n",
      "\n",
      "[2708 rows x 1435 columns]\n",
      "[4 1 1 3 4 4 1 4 0 5 0 1 3 5 0 1 5 6 0 0]\n",
      "(array([675.,   0., 376.,   0.,  94.,   0., 414.,   0., 327.,   0., 445.,\n",
      "         0., 377.]), array([0.        , 0.46153846, 0.92307692, 1.38461538, 1.84615385,\n",
      "       2.30769231, 2.76923077, 3.23076923, 3.69230769, 4.15384615,\n",
      "       4.61538462, 5.07692308, 5.53846154, 6.        ]), <BarContainer object of 13 artists>)\n",
      "(array([818.,   0., 180.,   0., 217.,   0., 426.,   0., 351.,   0., 418.,\n",
      "         0., 298.]), array([0.        , 0.46153846, 0.92307692, 1.38461538, 1.84615385,\n",
      "       2.30769231, 2.76923077, 3.23076923, 3.69230769, 4.15384615,\n",
      "       4.61538462, 5.07692308, 5.53846154, 6.        ]), <BarContainer object of 13 artists>)\n",
      "[0.32592593 0.15425532 0.27659574 0.23429952 0.14678899 0.17977528\n",
      " 0.76657825]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (7,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m7\u001b[39m):\n\u001b[1;32m     66\u001b[0m     hungarian\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mbincount(yhats[y\u001b[39m==\u001b[39mk]))\n\u001b[0;32m---> 67\u001b[0m _, hg_mapping \u001b[39m=\u001b[39m linear_sum_assignment(hungarian, maximize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m y_mapping: \u001b[39m# conversion ground truth labels\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     masked_labels \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlabels_\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (7,) + inhomogeneous part."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "from sklearn.metrics import adjusted_mutual_info_score as ari\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "f_edge = './data/cora.cites' # file location of the columns\n",
    "f_nodes = './data/cora.content' # file location of the rows\n",
    "\n",
    "# display function\n",
    "def plot_tSNE(labels_encoded,x_tsne):\n",
    "    color_map = np.argmax(labels_encoded, axis=1)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for cl in range(7):\n",
    "        indices = np.where(color_map==cl)\n",
    "        indices = indices[0]\n",
    "        plt.scatter(x_tsne[indices,0], x_tsne[indices, 1], label=cl)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def encode_label(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels, label_encoder.classes_\n",
    "\n",
    "\n",
    "\n",
    "C = 7 # nitiate desired kmeans cluster number\n",
    "V = 1433 # initiate vocabulary number\n",
    "word_col_names = [\"w{}\".format(i) for i in range(V)] # creates words column\n",
    "df_node = pd.read_csv(f_nodes, sep='\\t', header=None, names=['paperid']+word_col_names+['label']) # creates the matrix\n",
    "print(df_node) # displays the matrix\n",
    "\n",
    "X = df_node[word_col_names].to_numpy() # turns the matrix into a numpy array\n",
    "pca = PCA(30) # use pca\n",
    "data = pca.fit_transform(X) # makes the numpy array fit the pca specifications\n",
    "model = KMeans(n_clusters=C, random_state=None, init='random', n_init = 15, max_iter = 600)  # initiate the cluster with the desired parameters\n",
    "model.fit(data) # input the new transformed kmeans into the kmeans model\n",
    "print(model.labels_[:20]) # prints model label\n",
    "yhats = model.labels_\n",
    "np.unique(yhats) # list of the classes\n",
    "yhats\n",
    "print(plt.hist(yhats, bins='auto')) # prints the chart\n",
    "\n",
    "y, y_idx = pd.factorize(df_node['label']) # factorize the df_nodes\n",
    "labels_encoded, _ = encode_label(df_node['label'])\n",
    "y_mapping = {y_idx[k]: k for k in range(7)} # matching the index with the array count\n",
    "print(plt.hist(y, bins='auto')) # hart y chart post factorzation\n",
    "np.bincount(model.labels_[y==0]) # shows the number of Docs for each cluser label\n",
    "print(np.bincount(yhats[y==0])/np.bincount(yhats)) # true yhats over total yhats\n",
    "\n",
    "from sklearn.preprocessing import normalize # increase the accuracy by using normalize\n",
    "v = np.bincount(yhats[y==0])/np.bincount(yhats)\n",
    "normalized_v = normalize(v[:,np.newaxis], axis=0).ravel()\n",
    "hungarian = [] # hungarian algorithm finds mapping between cluster numbers and gtlabels using linear_sum_assignment\n",
    "for k in range(7):\n",
    "    hungarian.append(np.bincount(yhats[y==k]))\n",
    "_, hg_mapping = linear_sum_assignment(hungarian, maximize=True)\n",
    "for k in y_mapping: # conversion ground truth labels\n",
    "    masked_labels = model.labels_\n",
    "    best_cls = np.argmax(\n",
    "        np.bincount(yhats[y == y_mapping[k]]) / np.bincount(yhats)\n",
    "    )\n",
    "\n",
    "print(\"Acc: {}\".format(acc([hg_mapping[i] for i in y], yhats))) # printing accuracy\n",
    "# X_embedded = TSNE(n_components=2, verbose=1, perplexity=40).fit_transform(X)\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "plot_tSNE(labels_encoded, X_embedded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acc = \n",
    "\n",
    "<img src=\"./kmeans.png\" alt=\"ç¤ºä¾‹å›¾ç‰‡\" width=\"500\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
